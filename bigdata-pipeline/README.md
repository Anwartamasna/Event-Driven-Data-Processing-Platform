# Big Data Pipeline - User Event Processing

A production-style Docker-based big data pipeline that simulates user events, ingests them via Kafka, processes with Spark Streaming, aggregates with Spark Batch, and orchestrates with Airflow.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kafka     â”‚     â”‚  Kafka  â”‚     â”‚ Spark Streaming  â”‚     â”‚   Data Lake     â”‚
â”‚  Producer   â”‚â”€â”€â”€â”€â–¶â”‚  Topic  â”‚â”€â”€â”€â”€â–¶â”‚   (Continuous)   â”‚â”€â”€â”€â”€â–¶â”‚  /data/raw/     â”‚
â”‚ (Events)    â”‚     â”‚         â”‚     â”‚                  â”‚     â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                      â”‚
                                                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              Airflow DAG (Daily)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Wait for Data  â”‚â”€â”€â”€â–¶â”‚ Spark Batch Job â”‚â”€â”€â”€â–¶â”‚ Validate Results     â”‚      â”‚
â”‚  â”‚   (Sensor)     â”‚    â”‚  (Aggregation)  â”‚    â”‚   (Quality Check)    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   Data Lake     â”‚
                          â”‚ /data/processed/â”‚
                          â”‚ (Daily Metrics) â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
bigdata-pipeline/
â”œâ”€â”€ docker-compose.yml          # All services configuration
â”œâ”€â”€ README.md                   # This file
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ Dockerfile              # Custom Airflow image with Spark
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ daily_batch_pipeline.py  # Airflow DAG
â”‚
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ streaming_job.py        # Spark Structured Streaming
â”‚   â””â”€â”€ batch_job.py            # Spark Batch Aggregation
â”‚
â”œâ”€â”€ kafka/
â”‚   â””â”€â”€ producer.py             # Event generator
â”‚
â””â”€â”€ data/
    â”œâ”€â”€ raw/                    # Raw events (from streaming)
    â””â”€â”€ processed/              # Daily metrics (from batch)
```

## ğŸ› ï¸ Tech Stack

| Component          | Tool                              |
|--------------------|-----------------------------------|
| Message Broker     | Apache Kafka (Confluent)          |
| Stream Processing  | Spark Structured Streaming 3.5    |
| Batch Processing   | Apache Spark 3.5                  |
| Orchestration      | Apache Airflow 2.8                |
| Storage            | Local Filesystem (Data Lake)      |
| Containers         | Docker + Docker Compose           |

## ğŸš€ Quick Start

### Step 1: Start All Services

```bash
cd bigdata-pipeline
docker-compose up -d
```

Wait for all services to be ready (about 1-2 minutes):

```bash
docker-compose ps
```

### Step 2: Initialize Airflow (First Time Only)

```bash
# Wait for airflow-init to complete
docker-compose logs airflow-init

# Access Airflow UI at http://localhost:8081
# Login: admin / admin
```

### Step 3: Create Kafka Topic

```bash
docker exec kafka kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --topic user_events \
  --partitions 3 \
  --replication-factor 1
```

### Step 4: Start Spark Streaming Job (Run Once)

This job runs continuously in the background:

```bash
docker exec spark-master spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  /spark/streaming_job.py
```

> **Note:** Keep this terminal open. The streaming job writes data to `/data/raw/`.

### Step 5: Start Kafka Producer

In a **new terminal**, start the event producer:

```bash
# Option 1: Run inside Docker (requires Python in Kafka container)
docker exec -it kafka pip install kafka-python && \
docker exec -it kafka python /kafka/producer.py

# Option 2: Run locally (requires kafka-python installed)
cd bigdata-pipeline/kafka
pip install kafka-python
python producer.py --bootstrap-servers localhost:9092
```

### Step 6: Verify Data Flow

Check that raw data is being written:

```bash
ls -la data/raw/user_events/
```

### Step 7: Trigger Airflow DAG (Optional)

The DAG runs daily at 2 AM. To trigger manually:

1. Go to http://localhost:8081
2. Find `daily_batch_pipeline`
3. Toggle ON and click "Trigger DAG"

Or via CLI:

```bash
docker exec airflow-scheduler airflow dags trigger daily_batch_pipeline
```

## ğŸ“Š Sample Event Format

```json
{
  "user_id": 42,
  "event_type": "click",
  "timestamp": "2025-01-10T10:00:00",
  "page_id": 15,
  "session_id": "sess_12345"
}
```

**Event Types:**
- `click` (40%)
- `view` (35%)
- `purchase` (5%)
- `login` (10%)
- `logout` (10%)

## ğŸ” Accessing Services

| Service          | URL                        | Credentials   |
|------------------|----------------------------|---------------|
| Airflow UI       | http://localhost:8081      | admin / admin |
| Spark Master UI  | http://localhost:8080      | -             |
| Kafka            | localhost:9092             | -             |

## ğŸ§ª Testing the Pipeline

### Run Batch Job Manually

```bash
# Process today's date
docker exec spark-master spark-submit /spark/batch_job.py

# Process specific date
docker exec spark-master spark-submit /spark/batch_job.py --date 2025-01-15
```

### View Processed Metrics

```bash
ls -la data/processed/daily_metrics/
```

## ğŸ›‘ Shutdown

```bash
docker-compose down

# To remove volumes (data)
docker-compose down -v
```

## ğŸ“ Interview Demo Points

âœ… **Airflow UI** shows DAG success with green tasks  
âœ… **Kafka topic** receives events in real-time  
âœ… **Spark streaming** writes raw data continuously  
âœ… **Batch job** produces aggregated daily metrics  
âœ… **Production patterns**: sensor, validation, error handling  

## ğŸ”§ Troubleshooting

### Kafka Connection Issues
```bash
# Check Kafka is running
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092
```

### Spark Streaming Not Writing Data
```bash
# Check Spark logs
docker logs spark-master
```

### Airflow DAG Not Visible
```bash
# Restart scheduler
docker-compose restart airflow-scheduler
```

---

## License

MIT
